# My-notes

## Artificial Intelligence: 
The field of computer science that seeks o create 
intelligent machines that can replciate or exceed human intelligence.
## Machine Learning
Subset of AI that enables macines to learn from existing data
and improve upon that data to mke decision or predictions
## Deep Learning
A machine learning tecnique in wich layers of neural networs are used to process
data and make decisions

## Generative AI=LLM
Create new written, visual and auditory content given prompts or existing data
Key conept: tokanisation
receive text, give text
transform text to tokens(#s)

## What are language models?
Generative AI applications are powered by language models, which are a specialized type of machine learning model that you can use to perform natural language processing (NLP) tasks, including:
- Determining sentiment or otherwise classifying natural language text.
- Summarizing text.
- Comparing multiple text sources for semantic similarity.
- Generating new natural language.
While the mathematical principles behind these language models can be complex, a basic understanding of the architecture used to implement them can help you gain a conceptual understanding of how they work.

## Transformer models
Machine learning models for natural language processing have evolved over many years. Today's cutting-edge large language models are based on the transformer architecture, which builds on and extends some techniques that have been proven successful in modeling vocabularies to support NLP tasks - and in particular in generating language. Transformer models are trained with large volumes of text, enabling them to represent the semantic relationships between words and use those relationships to determine probable sequences of text that make sense. Transformer models with a large enough vocabulary are capable of generating language responses that are tough to distinguish from human responses.

## Transformer model architecture consists of two components, or blocks:
- An encoder block that creates semantic representations of the training vocabulary.
- A decoder block that generates new language sequences.

1. The model is trained with a large volume of natural language text, often sourced from the internet or other public sources of text.
2. The sequences of text are broken down into tokens (for example, individual words) and the encoder block processes these token sequences using a technique called attention to determine relationships between tokens (for example, which tokens influence the presence of other tokens in a sequence, different tokens that are commonly used in the same context, and so on.)
3. The output from the encoder is a collection of vectors (multi-valued numeric arrays) in which each element of the vector represents a semantic attribute of the tokens. These vectors are referred to as embeddings.
4. The decoder block works on a new sequence of text tokens and uses the embeddings generated by the encoder to generate an appropriate natural language output.
5. For example, given an input sequence like "When my dog was", the model can use the attention technique to analyze the input tokens and the semantic attributes encoded in the embeddings to predict an appropriate completion of the sentence, such as "a puppy."

In practice, the specific implementations of the architecture vary – for example, the Bidirectional Encoder Representations from Transformers (BERT) model developed by Google to support their search engine uses only the encoder block, while the Generative Pretrained Transformer (GPT) model developed by OpenAI uses only the decoder block.
While a complete explanation of every aspect of transformer models is beyond the scope of this module, an explanation of some of the key elements in a transformer can help you get a sense for how they support generative AI.

Tokenization
The first step in training a transformer model is to decompose the training text into tokens - in other words, identify each unique text value. For the sake of simplicity, you can think of each distinct word in the training text as a token (though in reality, tokens can be generated for partial words, or combinations of words and punctuation).

For example, consider the following sentence:
I heard a dog bark loudly at a cat
To tokenize this text, you can identify each discrete word and assign token IDs to them. For example:

```
- I (1)
- heard (2)
- a (3)
- dog (4)
- bark (5)
- loudly (6)
- at (7)
- *("a" is already tokenized as 3)*
- cat (8)
```

The sentence can now be represented with the tokens: {1 2 3 4 5 6 7 3 8}. Similarly, the sentence "I heard a cat" could be represented as {1 2 3 8}.

As you continue to train the model, each new token in the training text is added to the vocabulary with appropriate token IDs:

meow (9)
skateboard (10)
and so on...
With a sufficiently large set of training text, a vocabulary of many thousands of tokens could be compiled.


## Embeddings
While it may be convenient to represent tokens as simple IDs - essentially creating an index for all the words in the vocabulary, they don't tell us anything about the meaning of the words, or the relationships between them. To create a vocabulary that encapsulates semantic relationships between the tokens, we define contextual vectors, known as embeddings, for them. Vectors are multi-valued numeric representations of information, for example [10, 3, 1] in which each numeric element represents a particular attribute of the information. For language tokens, each element of a token's vector represents some semantic attribute of the token. The specific categories for the elements of the vectors in a language model are determined during training based on how commonly words are used together or in similar contexts.

Vectors represent lines in multidimensional space, describing direction and distance along multiple axes (you can impress your mathematician friends by calling these amplitude and magnitude). It can be useful to think of the elements in an embedding vector for a token as representing steps along a path in multidimensional space. For example, a vector with three elements represents a path in 3-dimensional space in which the element values indicate the units traveled forward/back, left/right, and up/down. Overall, the vector describes the direction and distance of the path from origin to end.

The elements of the tokens in the embeddings space each represent some semantic attribute of the token, so that semantically similar tokens should result in vectors that have a similar orientation – in other words they point in the same direction. A technique called cosine similarity is used to determine if two vectors have similar directions (regardless of distance), and therefore represent semantically linked words. As a simple example, suppose the embeddings for our tokens consist of vectors with three elements, for example:

- 4 ("dog"): [10,3,2]
- 8 ("cat"): [10,3,1]
- 9 ("puppy"): [5,2,1]
- 10 ("skateboard"): [-3,3,2]

```
 Note

The previous example shows a simple example model in which each embedding has only three dimensions. Real language models have many more dimensions.
```

There are multiple ways you can calculate appropriate embeddings for a given set of tokens, including language modeling algorithms like Word2Vec or the encoder block in a transformer model.

## Attention
The encoder and decoder blocks in a transformer model include multiple layers that form the neural network for the model. We don't need to go into the details of all these layers, but it's useful to consider one of the types of layers that is used in both blocks: attention layers. Attention is a technique used to examine a sequence of text tokens and try to quantify the strength of the relationships between them. In particular, self-attention involves considering how other tokens around one particular token influence that token's meaning.

In an encoder block, each token is carefully examined in context, and an appropriate encoding is determined for its vector embedding. The vector values are based on the relationship between the token and other tokens with which it frequently appears. This contextualized approach means that the same word might have multiple embeddings depending on the context in which it's used - for example "the bark of a tree" means something different to "I heard a dog bark."

In a decoder block, attention layers are used to predict the next token in a sequence. For each token generated, the model has an attention layer that takes into account the sequence of tokens up to that point. The model considers which of the tokens are the most influential when considering what the next token should be. For example, given the sequence “I heard a dog,” the attention layer might assign greater weight to the tokens “heard” and “dog” when considering the next word in the sequence:

I heard a dog [bark]

Remember that the attention layer is working with numeric vector representations of the tokens, not the actual text. In a decoder, the process starts with a sequence of token embeddings representing the text to be completed. The first thing that happens is that another positional encoding layer adds a value to each embedding to indicate its position in the sequence:

```
[1,5,6,2] (I)
[2,9,3,1] (heard)
[3,1,1,2] (a)
[4,10,3,2] (dog)
```
During training, the goal is to predict the vector for the final token in the sequence based on the preceding tokens. The attention layer assigns a numeric weight to each token in the sequence so far. It uses that value to perform a calculation on the weighted vectors that produces an attention score that can be used to calculate a possible vector for the next token. In practice, a technique called multi-head attention uses different elements of the embeddings to calculate multiple attention scores. A neural network is then used to evaluate all possible tokens to determine the most probable token with which to continue the sequence. The process continues iteratively for each token in the sequence, with the output sequence so far being used regressively as the input for the next iteration – essentially building the output one token at a time.

1. A sequence of token embeddings is fed into the attention layer. Each token is represented as a vector of numeric values.
2. The goal in a decoder is to predict the next token in the sequence, which will also be a vector that aligns to an embedding in the model’s vocabulary.
3. The attention layer evaluates the sequence so far and assigns weights to each token to represent their relative influence on the next token.
4. The weights can be used to compute a new vector for the next token with an attention score. Multi-head attention uses different elements in the embeddings to calculate multiple alternative tokens.
5. A fully connected neural network uses the scores in the calculated vectors to predict the most probable token from the entire vocabulary.
6. The predicted output is appended to the sequence so far, which is used as the input for the next iteration.

During training, the actual sequence of tokens is known – we just mask the ones that come later in the sequence than the token position currently being considered. As in any neural network, the predicted value for the token vector is compared to the actual value of the next vector in the sequence, and the loss is calculated. The weights are then incrementally adjusted to reduce the loss and improve the model. When used for inferencing (predicting a new sequence of tokens), the trained attention layer applies weights that predict the most probable token in the model’s vocabulary that is semantically aligned to the sequence so far.

What all of this means, is that a transformer model such as GPT-4 (the model behind ChatGPT and Bing) is designed to take in a text input (called a prompt) and generate a syntactically correct output (called a completion). In effect, the “magic” of the model is that it has the ability to string a coherent sentence together. This ability doesn't imply any “knowledge” or “intelligence” on the part of the model; just a large vocabulary and the ability to generate meaningful sequences of words. What makes a large language model like GPT-4 so powerful however, is the sheer volume of data with which it has been trained (public and licensed data from the Internet) and the complexity of the network. This enables the model to generate completions that are based on the relationships between words in the vocabulary on which the model was trained; often generating output that is indistinguishable from a human response to the same prompt.

## Using language models

Organizations and developers can train their own language models from scratch, but in most cases it’s more practical to use an existing foundation model, and optionally fine-tune it with your own training data. There are many sources of model that you can use.

On Microsoft Azure, you can find foundation models in the Azure OpenAI service and in the Model Catalog. The Model Catalog is a curated source of models for data scientists and developers using Azure AI Studio and Azure Machine Learning. This offers the benefit of cutting-edge language models like the generative pre-trained transformer (GPT) collection of models (on which ChatGPT and Microsoft's own generative AI services are based) as well as the DALL-E model for image generation. Using these models from the Azure OpenAI service means that you also get the benefit of a secure, scalable Azure cloud platform in which the models are hosted.

In addition to the Azure OpenAI models, the model catalog includes the latest open-source models from Microsoft and multiple partners, including:

- OpenAI
- HuggingFace
- Mistral
- Meta and others.

## Large and small language models
There are many language models available that you can use to power generative AI applications. In general, language models can be considered in two categorize: Large Language Models (LLMs) and Small Language models (SLMs).

**Large Language Models (LLMs)**

- LLMs are trained with vast quantities of text that represents a wide range of general subject matter – typically by sourcing data from the Internet and other generally available publications.

- When trained, LLMs have many billions (even trillions) of parameters (weights that can be applied to vector embeddings to calculate predicted token sequences).

- Able to exhibit comprehensive language generation capabilities in a wide range of conversational contexts.

- Their large size can impact their performance and make them difficult to deploy locally on devices and computers.

- Fine-tuning the model with additional data to customize its subject expertise can be time-consuming, and expensive in terms of the compute power required to perform the additional training.

**Small Language Models (SLMs)**

- SLMs are trained with smaller, more subject-focused datasets

- Typically have fewer parameters than LLMs.

- This focused vocabulary makes them very effective in specific conversational topics, but less effective at more general language generation.

- The smaller size of SLMs can provide more options for deployment, including local deployment to devices and on-premises computers; and makes them faster and easier to fine-tune.

- Fine-tuning can potentially be less time-consuming and expensive.

## Copilots
The availability of language models has led to the emergence of new ways to interact with applications and systems through digital copilots. Copilots are generative AI assistants that are integrated into applications often as chat interfaces. They provide contextualized support for common tasks in those applications.

Microsoft Copilot is integrated into a wide range of Microsoft applications and user experiences. It is based on an open architecture that enables third-party developers to create their own plug-ins to extend or customize the user experience with Microsoft Copilot. Additionally, third-party developers can create their own copilots using the same open architecture.

Business users can use copilots to boost their productivity and creativity with AI-generated content and automation of tasks. Developers can extend copilots by creating plug-ins that integrate them into business processes and data, or even create custom copilots to build generative AI capabilities into apps and services.

Copilots have the potential to revolutionize the way we work by helping with first drafts, information synthesis, strategic planning, and much more. The goal of copilot features is to empower people to be smarter, more productive, more creative, and connected to the people and things around them.

**Levels of Copilot adoption**
In general, you can categorize industry and personal Copilot adoption into three buckets: off-the-shelf use, extending Microsoft Copilot, and custom development.

- You can use off-the-shelf copilots, like Microsoft Copilot for Microsoft 365 to empower users and increase their productivity.
- You can extend Microsoft Copilot to support custom business processes or tasks, using your own data to control how Copilot responds to user prompts in your organization.
- You can build custom copilots to integrate generative AI into business apps or to create unique experiences for your customers.

## Microsoft Copilot
Microsoft Copilot features can be found throughout all different Microsoft applications. They unlock productivity across your organization, safeguard your business, and build and extend your AI capabilities. Explore some of the different use cases for Microsoft Copilot below.

**Web browsing with AI**
Microsoft Copilot: use Microsoft Copilot to answer questions, create content, and search the web with the Microsoft Copilot app at https://copilot.microsoft.com, when using the Bing search engine, and in the Edge browser. For example, you can ask Microsoft Copilot to generate a list of opportunities in an industry or give more detailed information from your search results. When you browse with Microsoft Edge, Copilot is built right in. You can open the Copilot pane in the browser and use it to research topics and create new content – for example to publish a blog post. With all of these Copilot options, signing in with a work or school account enables you to use Copilot in the context of your organization’s data and services – enabling you to get assistance with internal resources and information.

**AI assistance for information workers**
Copilot for Microsoft 365: Microsoft Copilot for Microsoft 365 integrates Copilot into the productivity applications that information workers use every day. You can use Copilot in Microsoft Word to generate a new document based on a natural language prompt, and then refine, summarize, and improve the document with a few prompts.

You can use Copilot in Microsoft PowerPoint to create a whole presentation based on the contents of a document or email, and then add graphics, reformat slides, and otherwise improve your presentation.

In Microsoft Outlook, Copilot can help you summarize your emails, check your schedule, and even find relevant emails and documents to prepare for meetings.

These are just some examples of how you can use Microsoft Copilot for Microsoft 365. There’s lots more you can accomplish in Windows, Excel, Teams, and other apps. Learn more at https://www.microsoft.com/microsoft-365/enterprise/copilot-for-microsoft-365

**Use AI to support business processes**
Copilot in Dynamics 365 Customer Service: Modernizes contact centers with generative AI.|Customer service agents use Copilot to analyze support tickets, research similar issues, find resolutions, and communicate them to users with only a few clicks and prompts.

Copilot for Dynamics 365 Sales: Sales professionals can use Copilot to quickly find relevant customer and industry information by integrating with the company’s customer relationship management (CRM) database and beyond. This can enable an account manager to quickly review and qualify a lead, generate a proposal, and set up a customer engagement to close the deal.

Copilot for Dynamics 365 Supply Chain: Handles changes to purchase orders at scale and assess the impact and risk to help optimize procurement decisions. For example, Copilot identifies the level of impact that changes to purchase orders have on downstream processes and gives advice for next steps.

AI assisted data analytics
Copilot in Microsoft Fabric: Copilot enables analysts to automatically generate the code they need to analyze, manipulate, and visualize data in Spark notebooks.

Copilot in Power BI: When creating Power BI reports, Copilot can analyze your data and then suggest and create appropriate data visualizations from it.

Manage IT infrastructure and security
Microsoft Copilot for Security: Provides assistance for security professionals as the assess, mitigate, and respond to security threats.

Microsoft Copilot for Azure: Integrated into the Azure portal to assist infrastructure administrators as they work with Azure cloud services.

AI powered software development
GitHub Copilot: Helps developers maximize their productivity by analyzing and explaining code, adding code documentation, generating new code based on natural language prompts, and more.

## Considerations for Copilot prompts
The quality of responses from copilots not only depends on the language model used, but on the types of prompts users provide. Prompts are ways we tell an application what we want it to do. You can get the most useful completions by being explicit about the kind of response you want. Take this example, "Summarize the key considerations for adopting Copilot described in this document for a corporate executive. Format the summary as no more than six bullet points with a professional tone." You can achieve better results when you submit clear, specific prompts.

Consider the following ways you can improve the response a copilot provides:
1. Start with a specific goal for what you want the copilot to do
2. Provide a source to ground the response in a specific scope of information
3. Add context to maximize response appropriateness and relevance
4. Set clear expectations for the response
5. Iterate based on previous prompts and responses to refine the result

In most cases, a copilot doesn't just send your prompt as-is to the language model. Usually, your prompt is augmented with:

. A system message that sets conditions and constraints for the language model behavior. For example, "You're a helpful assistant that responds in a cheerful, friendly manner." These system messages determine constraints and styles for the model's responses.
. The conversation history for the current session, including past prompts and responses. The history enables you to refine the response iteratively while maintaining the context of the conversation.
. The current prompt – potentially optimized by the copilot to reword it appropriately for the model or to add more grounding data to scope the response.

The term prompt engineering describes the process of prompt improvement. Both developers who design applications and consumers who use those applications can improve the quality of responses from generative AI by considering prompt engineering.

## Extending and developing copilots
If your organization makes the decision to customize Microsoft copilot or develop custom copilots, Microsoft provides two tools that you can use, Copilot Studio and Azure AI Studio.

**Copilot Studio**
Copilot Studio is designed to work well for low-code development scenarios in which technically proficient business users or developers can create conversational AI experiences. The resulting copilot is a fully managed SaaS (software as a service) solution, hosted in your Microsoft 365 environment and delivered through chat channels like Microsoft Teams. With Copilot Studio, the infrastructure considerations and model deployment details are taken care of for you, making it easy to focus on creating an effective solution. For more information, see https://www.microsoft.com/microsoft-copilot/microsoft-copilot-studio.

**Azure AI Studio**
Azure AI Studio is a PaaS (platform as a service) development portal for professional software developers that gives you full control over the language model you want to use, including the capability to fine-tune the model with your own data. You can define prompt flows that orchestrate conversation flow and integrate your own data augmentation and prompt engineering logic, and you can deploy the resulting copilot service in the cloud and consume it from custom-developed apps and services. For more information, see https://azure.microsoft.com/products/ai-studio/.

**Summary**
Generative AI is a rapidly developing field of AI that supports new language generation, code development, image creation, and more. This module explored how language models power generative AI and how copilot brings generative AI capabilities to your desktop. You learned the basics of Large Language Models (LLMs) and Small Language Models (SLMs), and the underlying transformer architecture of advanced language models.

The main takeaways from this module include understanding the role of generative AI in creating AI assistants or copilots that provide contextualized support for common tasks. Often, generative AI is integrated into chat applications like Microsoft Copilot to interpret natural language inputs and generate suitable responses. You learned about the three levels of copilot adoption and the various applications of Microsoft Copilot across different Microsoft applications. The module also highlighted the importance of improving the quality of responses from generative AI. Finally, you have been introduced to two tools offered by Microsoft for customizing or developing custom copilots: Copilot Studio and Azure AI Studio.




